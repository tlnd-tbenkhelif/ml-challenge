{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Please enter your team name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_name = input(\"Please enter your team name: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "\n",
    "\n",
    "# Can you predict the movies success ?\n",
    "![](https://media.giphy.com/media/3ohhwDMC187JqL69DG/giphy.gif)\n",
    "\n",
    "In this tutorial we will go through some basic steps of machine learning model creation, to get familiar with this topic on a real life dataset.\n",
    "\n",
    "\n",
    "At the end of this tutorial you will discover:\n",
    "\n",
    "1- How to explore and manipulate a dataset?\n",
    "\n",
    "2- How to work with famous machine learning libraries like Scikit-learn?\n",
    "\n",
    "3- How does feature engineering process work?\n",
    "\n",
    "4- How to model a real life problem by Machine Learning algorithm?\n",
    "\n",
    "\n",
    "Keep in mind that **YOUR** results will help to improve marketing efforts in the film industry and predicting if the film is going to show ducks or miracle in the cinema after release.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## The Movies Dataset\n",
    "\n",
    "The dataset comes from the IMDB API, it consists of 26 million ratings on around 45,000 movies from 27,000 users.\n",
    "The aim is to be able to create a regression model that predict the average rate of each movie. The exact number of rows are 45466 with 20 columns that are divided in two parts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  The features are\n",
    "* **belongs_to_collection:** A stringified dictionary that gives information on the movie series the particular film belongs to.\n",
    "* **budget:** The budget of the movie in dollars.\n",
    "* **genres:** A stringified list of dictionaries that list out all the genres associated with the movie.\n",
    "* **homepage:** The Official Homepage of the move.\n",
    "* **id:** The ID of the move.\n",
    "* **original_language:** The language in which the movie was originally shot in.\n",
    "* **original_title:** The original title of the movie.\n",
    "* **overview:** A brief blurb of the movie.\n",
    "* **popularity:** The Popularity Score assigned by IMDB.\n",
    "* **production_companies:** A stringified list of production companies involved with the making of the movie.\n",
    "* **production_countries:** A stringified list of countries where the movie was shot/produced in.\n",
    "* **revenue:** The total revenue of the movie in dollars.\n",
    "* **spoken_languages:** A stringified list of spoken languages in the film.\n",
    "* **title:** The Official Title of the movie.\n",
    "* **vote_average:** The average rating of the movie.\n",
    "* **vote_count:** The number of votes by users, as counted by IMDB.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine learning pipeline\n",
    "\n",
    "![title](img/pipeLineML2.png)\n",
    "\n",
    "**1. Steps to create a machine learning pipeline:**\n",
    "* Data preparation:\n",
    "  * Data Cleaning\n",
    "    * Irrelevant data\n",
    "    * Duplicated rows\n",
    "    * Missing values\n",
    "    \n",
    "  * Feature Engineering\n",
    "    * Create new features\n",
    "    * Convert features to appropriate format\n",
    "* Training the model\n",
    "  * Choosing a model\n",
    "  * Tune it's parameters \n",
    "* Predicting\n",
    "  * Predict on unseen data\n",
    "  \n",
    "**2. Regression Models:** Predict a numerical value given a set of features\n",
    "![title](img/Reg.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A good fitting tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/biais.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use a validation set to avoid over/underfitting "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/train-test-split.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Note:** If you have to pre-process data don't forget to do it for all the partitions Train, Validation and Test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Consider you need a loan. How will the bank know if you'll pay it back or not? The bank has lots of profiles of people who took money before. They have data about age, education, occupation and salary and – most importantly – the fact of paying the money back Or not. Using this data, we can teach the machine to find the patterns and get the answer.**\n",
    "\n",
    "![input](img/loan.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From trees to forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/RForest.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RF consists of a large number of individual randomized decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to predict the average rate for each film by means of RF.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all libraries\n",
    "\n",
    "import pandas as pd                         #  easy-to-use data structures \n",
    "import numpy as np                          #  For multidimensional array objects\n",
    "from math import sqrt\n",
    "\n",
    "import matplotlib.pyplot as plt             #  To create plots\n",
    "import seaborn as sns\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS  # word cloud generator\n",
    "from IPython.display import HTML\n",
    "from IPython.display import display\n",
    "\n",
    "#Scikit learn library\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "\n",
    "\n",
    "#to run the cell press the \"Run\" or \"shift + Enter\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the data\n",
    "#The path to datasets\n",
    "PATH = \"data\"\n",
    "\n",
    "\n",
    "feature = pd.read_csv(PATH + '/Train.csv')    \n",
    "target = pd.read_csv(PATH + '/Y_train.csv')   \n",
    "\n",
    "test = pd.read_csv(PATH + '/Test.csv')      # The  set that we will predict the target\n",
    "\n",
    "#For exploring text data\n",
    "text = pd.read_csv(PATH + '/text.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see what data can tell us. This important step is known as Exploratory data analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first 3 rows of the dataset are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "plt.scatter(feature.revenue, target.vote_average)\n",
    "plt.xlabel(\"Revenue\")\n",
    "plt.ylabel(\"Vote average\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **PART 1: Preprocessing**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature.info() #test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Irrelevant data:**\n",
    "those that are not actually needed, and don’t fit under the context of the problem we’re trying to solve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = feature.drop(['belongs_to_collection'], axis=1)\n",
    "test = test.drop(['belongs_to_collection'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling missing values\n",
    "\n",
    "**CASE 1**\n",
    "If the missing values in a column rarely happen and occur at random like \"production_countries\", \"popularity\" ,... then the easiest and most forward solution is to drop observations (rows) that have missing values. If most of the column’s values are missing, and occur at random, then a typical decision is to drop the whole column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature.isnull().sum().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.isnull().sum().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['release_date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Percentage of missing values in \"belongs_to_collection\"\n",
    "missing_value= feature[\"revenue\"].isnull().sum()\n",
    "print(f'The percentage of missing values in \"revenue\" is {round((missing_value/feature.shape[0]) * 100)} ')# round the value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize the missing value\n",
    "sns.heatmap(feature.isnull(), yticklabels = False, cbar = False, cmap = 'viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CASE 2**\n",
    "Imputation means calculate the missing value based on other observations by using statistical values like mean, median.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature['revenue'] = feature['revenue'].fillna(feature['revenue'].mean())\n",
    "test['revenue'] = test['revenue'].fillna(feature['revenue'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature['genres'] = feature['genres'].replace([], np.nan)\n",
    "test['genres'] = test['genres'].replace([], np.nan)\n",
    "#it can be done the same for the other features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "\n",
    "Feature engineering is the process of transforming raw data into features that better represent the underlying problem to the predictive models. \n",
    "\n",
    "Can you turn some of the features into things that the algorithm can understand?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converted into number of genres\n",
    "feature['len_genres'] = feature['genres'].apply(lambda x: len(x))\n",
    "test['len_genres'] = test['genres'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print the diffrent genres\n",
    "\n",
    "import ast\n",
    "list2 = [ast.literal_eval(item) for item in feature['genres'].unique()]\n",
    "def flatten(lst):\n",
    "    for el in lst:\n",
    "        if isinstance(el, list):\n",
    "            yield from el\n",
    "        else:\n",
    "            yield el\n",
    "\n",
    "list3 = flatten(list2)\n",
    "list(set(list3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast the genres column in to a list \n",
    "feature['genres_list'] = feature['genres'].apply(ast.literal_eval)\n",
    "test['genres_list'] = test['genres'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_comedy_in_the_list (x):\n",
    "    if 'Comedy' in x :\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lest add a binary column, that indicates if the movie is a comedy or not ?\n",
    "feature['is_comedy'] = feature['genres_list'].apply(is_comedy_in_the_list)\n",
    "test['is_comedy'] = test['genres_list'].apply(is_comedy_in_the_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "# TODO: You are now ready to add your own features or to modify the existing ones \n",
    "#\n",
    "#\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select features that will be used to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = ['len_genres','revenue','popularity','is_comedy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data randomly to the train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, Y_train, y_valid = train_test_split(feature[selected_columns], target,\n",
    "                                                      test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression problem: what is the average rate of each movie?\n",
    "\n",
    "Is it possible to predict the average vote of coming movies (unseen data)?\n",
    "\n",
    "To model the problem, we are going to use a Random Forest regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not hesitate to tweak those parameters: find the good tradeoff between underfitting and overfitting\n",
    "#\n",
    "#\n",
    "#\n",
    "RF = RandomForestRegressor(bootstrap=True, #  method for sampling data points (with or without replacement)                \n",
    "                           max_depth=None, #  max number of levels in each decision tree\n",
    "                           max_features='auto', #  max number of features considered for splitting a node\n",
    "                           min_samples_leaf=1,  #  min number of data points allowed in a leaf node\n",
    "                           min_samples_split=2, #  min number of data points placed in a node before the node is split\n",
    "                           n_estimators=10 # number of trees in the foreset\n",
    "                           )       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build a forest of trees from the training set\n",
    "RF.fit(X_train,Y_train['vote_average']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction =  RF.predict(X_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Root Mean Square Error:', np.round(sqrt(mean_squared_error(y_valid['vote_average'], prediction)),3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations  !\n",
    " \n",
    "You have now created a machine learning regression model using sklearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please save and submit your results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_prediction = RF.predict(test[selected_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\n",
    "        \"id\": test.id,\n",
    "        \"predictions\": np.round(submission_prediction, 3) \n",
    "})\n",
    "submission.to_csv(team_name +'.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Please submit your predictions file (team_name +'.csv' located in your working directory) to the following url:\n",
    "\n",
    "https://drive.google.com/drive/folders/1-doveGLiaPDGpAIzDM_JQ90TUXgZ5nX7?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are there certain words considered more worthy of a title or overviews?\n",
    "Some features are text based like \"title\" and \"overview\". Word cloud (text clouds or tag clouds) helps to highlight important textual data points. The more a specific word appears in a source of textual data, the bigger and bolder it appears in the word cloud.\n",
    "The only required argument for a WordCloud object is the text, while all others are optional.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert type of the \"title\" and \"overview\" features from object to string. It should be\n",
    "# considered that values in a particular column must be of a particular datatype, \n",
    "#e.g., boolean, numeric, date, etc.\n",
    "\n",
    "text['title'] = text['title'].astype('str')\n",
    "text['overview'] = text['overview'].astype('str')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a corpus of words\n",
    "title_corpus = ' '.join(text['title'])\n",
    "overview_corpus = ' '.join(text['overview'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overview_wordcloud = WordCloud(stopwords=STOPWORDS, background_color='white', height=2000, width=4000).generate(overview_corpus)\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.imshow(overview_wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_wordcloud = WordCloud(stopwords=STOPWORDS, background_color='white', height=2000, width=4000).generate(title_corpus)\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.imshow(title_wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
